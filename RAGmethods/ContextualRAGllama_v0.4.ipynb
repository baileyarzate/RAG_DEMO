{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if doing clean extraction from database, run this\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from unstructured.partition.auto import partition\n",
    "from langchain.schema import Document\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import pickle\n",
    "#from langchain.document_loaders import DirectoryLoader\n",
    "#from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# 1. Document Loading and Page Tracking\n",
    "docs = []\n",
    "doc_folder = r'C:\\Users\\Ian\\Documents\\RAG\\B1-B data'\n",
    "for filename in os.listdir(doc_folder):\n",
    "    filepath = os.path.join(doc_folder, filename)\n",
    "    if os.path.isfile(filepath):\n",
    "        elements = partition(filename=filepath)\n",
    "        for i, element in enumerate(elements):\n",
    "            # Extract text content and page information\n",
    "            text = str(element) \n",
    "            page_number = element.metadata.page_number if element.metadata.page_number else 'N/A'  # Extract page info\n",
    "            docs.append({\"source\": filename, \"content\": text, \"page\": page_number})\n",
    "\n",
    "# 2. Chunking while Preserving Page Information\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4096, chunk_overlap=300)\n",
    "all_splits = []\n",
    "current_chunk = \"\"\n",
    "current_metadata = {} \n",
    "\n",
    "for doc in docs:\n",
    "    splits = text_splitter.split_text(doc['content'])\n",
    "    for split in splits:\n",
    "        if len(current_chunk) + len(split) <= 4096: \n",
    "            current_chunk += split + \" \" # Add to the current chunk\n",
    "            current_metadata = {\"source\": doc['source'], \"page\": doc['page']} \n",
    "        else:\n",
    "            all_splits.append(Document(page_content=current_chunk, metadata=current_metadata))\n",
    "            current_chunk = split + \" \"\n",
    "            current_metadata = {\"source\": doc['source'], \"page\": doc['page']}\n",
    "\n",
    "# Append the last chunk\n",
    "if current_chunk:\n",
    "    all_splits.append(Document(page_content=current_chunk, metadata=current_metadata)) \n",
    "\n",
    "#save docs to pickle file\n",
    "with open(r'C:\\Users\\Ian\\Documents\\RAG\\B-1B data processed\\docs.pkl', 'wb') as f:\n",
    "    pickle.dump(docs, f)\n",
    "\n",
    "# 3. Vectorstore and Retriever Setup\n",
    "model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma.from_documents(all_splits, model, collection_name=\"B-1B\", persist_directory=r\"C:\\Users\\Ian\\Documents\\RAG\\B-1B data processed\")\n",
    "llm = ChatOllama(model=\"llama3.1:8b\")  # Or your preferred LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28689 documents.\n"
     ]
    }
   ],
   "source": [
    "# if docs.pkl already exists, run this -- run this in demo\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "from unstructured.partition.auto import partition\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import pickle\n",
    "# Load docs from pickle file\n",
    "with open(r'C:\\Users\\Ian\\Documents\\RAG\\B-1B data processed\\docs.pkl', 'rb') as f:\n",
    "    docs = pickle.load(f)\n",
    "\n",
    "# Optional: Display the loaded docs to verify\n",
    "print(f\"Loaded {len(docs)} documents.\")\n",
    "\n",
    "# Chunking while Preserving Page Information\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4096, chunk_overlap=300)\n",
    "all_splits = []\n",
    "current_chunk = \"\"\n",
    "current_metadata = {}\n",
    "\n",
    "for doc in docs:\n",
    "    splits = text_splitter.split_text(doc['content'])\n",
    "    for split in splits:\n",
    "        if len(current_chunk) + len(split) <= 4096:\n",
    "            current_chunk += split + \" \"\n",
    "            current_metadata = {\"source\": doc['source'], \"page\": doc['page']}\n",
    "        else:\n",
    "            all_splits.append(Document(page_content=current_chunk, metadata=current_metadata))\n",
    "            current_chunk = split + \" \"\n",
    "            current_metadata = {\"source\": doc['source'], \"page\": doc['page']}\n",
    "\n",
    "# Append the last chunk\n",
    "if current_chunk:\n",
    "    all_splits.append(Document(page_content=current_chunk, metadata=current_metadata))\n",
    "\n",
    "# Vectorstore and Retriever Setup\n",
    "embedding_function = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma.from_documents(all_splits, embedding_function, collection_name=\"B-1B\", persist_directory=r'C:\\Users\\Ian\\Documents\\RAG\\B-1B data processed\\vectorDB_B-1B')\n",
    "llm = ChatOllama(model=\"llama3.1:8b\")  # Or your preferred LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. RAG Function (Incorporating Contextual Compression)\n",
    "def RAG(user_prompt, llm, vectorstore, stream=False, source_summaries=False, retrieval = 'contextual', top_k_hits = 5):\n",
    "    # retrieval methods: contextual, cosine_similarity, both\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(\n",
    "            f\"Source: {doc.metadata['source']} - Page: {doc.metadata.get('page', 'N/A')}\\n\\n{doc.page_content}\" \n",
    "            for doc in docs\n",
    "        )\n",
    "\n",
    "    RAG_TEMPLATE = \"\"\"\n",
    "    This is a chat between a user and an AI assistant. The assistant provides detailed, polite answers based on the context provided.\n",
    "\n",
    "    Definitions:\n",
    "    - 'Embedded' content is directly from the PDF.\n",
    "    - 'Predicted' content is generated by an OCR model and may contain inaccuracies (e.g., spelling, spacing, symbols).\n",
    "\n",
    "    Each document is tracked by its source and page number. When answering, cite the source document and page number in parentheses, like this: ([Source: document_name, Page X]). If the answer comes from multiple chunks of the same document, reference all relevant chunks.\n",
    "\n",
    "    Indicate when information cannot be found in the context.\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Answer the following question:\n",
    "\n",
    "    {question}\n",
    "    \"\"\"\n",
    "    question = user_prompt\n",
    "    rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    if retrieval == 'contextual' or retrieval == 'both':\n",
    "        compressor = LLMChainExtractor.from_llm(llm) \n",
    "        compression_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=compressor, base_retriever=retriever\n",
    "        )\n",
    "        qa_chain = (\n",
    "            {\"context\": compression_retriever | format_docs, \"question\": RunnablePassthrough()} \n",
    "            | rag_prompt\n",
    "            | llm\n",
    "            | StrOutputParser() \n",
    "        )\n",
    "        \n",
    "        docs = compression_retriever.invoke(question)  # Invoke on the question \n",
    "    if retrieval == 'cosine similarity' or retrieval == 'both':\n",
    "        if retrieval != 'both':\n",
    "            qa_chain = (\n",
    "                {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "                | rag_prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "        if retrieval == 'both':\n",
    "            docs.extend(vectorstore.similarity_search(question, k = top_k_hits))\n",
    "        else:\n",
    "            docs = vectorstore.similarity_search(question, k = top_k_hits)\n",
    "\n",
    "    if not docs:\n",
    "        return \"No relevant documents found\", pd.DataFrame()\n",
    "\n",
    "    source_data = []\n",
    "    for doc in docs:\n",
    "        source_data.append({\n",
    "            \"source\": doc.metadata['source'], \n",
    "            \"end page\": doc.metadata.get('page', 'N/A'),\n",
    "            \"content\": doc.page_content \n",
    "        })\n",
    "\n",
    "    if source_summaries:\n",
    "        summaries = [llm.invoke(f'Summarize this in one or two sentences. Only state main point, nothing else. <{doc.page_content}> ').content for doc in docs]\n",
    "        source_df = pd.DataFrame(source_data)\n",
    "        source_df[\"short summary\"] = summaries\n",
    "    else:\n",
    "        source_df = pd.DataFrame(source_data)\n",
    "\n",
    "    if stream:\n",
    "        for chunk in qa_chain.stream(question):\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "        return '', source_df\n",
    "    else:\n",
    "        result = qa_chain.invoke(question)\n",
    "        return result, source_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def RAG_gradio(user_prompt, retrieval_method, top_k_hits=5): \n",
    "    result, sources_df = RAG(user_prompt, llm, vectorstore, retrieval=retrieval_method, top_k_hits=top_k_hits)\n",
    "\n",
    "    # Format source information for display\n",
    "    root_dir = doc_folder\n",
    "    doc_names = sources_df['source'].values\n",
    "    sources_df['source'] = sources_df['source'].apply(lambda x: root_dir + '\\\\' + x)\n",
    "    # Return the result and HTML representation of the DataFrame\n",
    "    return result, sources_df.to_html(escape=False)\n",
    "\n",
    "# Gradio Interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### B-1B Database Q&A\")\n",
    "\n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(label=\"Enter your question:\")\n",
    "        retrieval_choice = gr.Radio(\n",
    "            choices=[\"contextual\", \"cosine similarity\", \"both\"],\n",
    "            label=\"Retrieval Method:\",\n",
    "            value=\"cosine similarity\"\n",
    "        )\n",
    "        \n",
    "        top_k_slider = gr.Slider(minimum=1, maximum=10, step=1, value=3, label=\"Cosine Similarity Number of Documents:\")\n",
    "        \n",
    "    output = gr.Markdown(label=\"Answer:\")  # Set interactive=False for the answer\n",
    "    source_output = gr.HTML(label = 'Source Information')  # Changed to gr.HTML to display HTML content\n",
    "\n",
    "    # Function to update slider visibility based on the retrieval method\n",
    "    def update_slider(method):\n",
    "        if method == \"contextual\":\n",
    "            return gr.update(visible=True, value=1), gr.update(label=\"Not Applicable to Contexual Retrieval\")  # Hide the slider\n",
    "        elif method == \"cosine similarity\":\n",
    "            return gr.update(visible=True, value=5), gr.update(label=\"Cosine Similarity Number of Documents:\")  # Show slider with new label\n",
    "        elif method == \"both\":\n",
    "            return gr.update(visible=True, value=5), gr.update(label=\"Cosine Similarity Number of Documents (not contextual):\")  # Show slider with new label\n",
    "\n",
    "    # Add a change event to the retrieval_choice radio button\n",
    "    retrieval_choice.change(update_slider, inputs=retrieval_choice, outputs=[top_k_slider, top_k_slider])\n",
    "\n",
    "    def show_thinking():\n",
    "        return \"Processing your request...\"  # Placeholder to indicate \"thinking\"\n",
    "    \n",
    "    btn = gr.Button(\"Submit\")\n",
    "    btn.click(\n",
    "        fn=show_thinking,  # First show \"thinking\" message\n",
    "        inputs=None,\n",
    "        outputs=output\n",
    "    )\n",
    "    \n",
    "    # Simulate query after showing \"thinking\" state\n",
    "    btn.click(\n",
    "        fn=RAG_gradio, \n",
    "        inputs=[user_input, retrieval_choice, top_k_slider],\n",
    "        outputs=[output, source_output]\n",
    "    )\n",
    "\n",
    "demo.launch(share = False)#inbrowser = False, #opens gradio in interface in browser when ran (T/F)\n",
    "            #inline = False, #Shows on external window or not (T/F)\n",
    "            #share = False, #make a public website (T/F)\n",
    "            #auth = None) #to add a password, use this argument: auth=(\"admin\", \"pass1234\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
