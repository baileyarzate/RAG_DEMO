{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever, MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from unstructured.partition.auto import partition\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "import pandas as pd\n",
    "# 1. Document Loading and Page Tracking\n",
    "docs = []\n",
    "doc_folder = r'C:\\Users\\admin\\Documents\\LLM\\B1-B data'\n",
    "for filename in os.listdir(doc_folder):\n",
    "    filepath = os.path.join(doc_folder, filename)\n",
    "    if os.path.isfile(filepath):\n",
    "        elements = partition(filename=filepath)\n",
    "        for i, element in enumerate(elements):\n",
    "            # Extract text content and page information\n",
    "            text = str(element) \n",
    "            page_number = element.metadata.page_number if element.metadata.page_number else 'N/A'  # Extract page info\n",
    "            docs.append({\"source\": filename, \"content\": text, \"page\": page_number})\n",
    "\n",
    "# 2. Chunking while Preserving Page Information\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4096, chunk_overlap=300)\n",
    "all_splits = []\n",
    "current_chunk = \"\"\n",
    "current_metadata = {} \n",
    "\n",
    "for doc in docs:\n",
    "    splits = text_splitter.split_text(doc['content'])\n",
    "    for split in splits:\n",
    "        if len(current_chunk) + len(split) <= 4096: \n",
    "            current_chunk += split + \" \" # Add to the current chunk\n",
    "            current_metadata = {\"source\": doc['source'], \"page\": doc['page']} \n",
    "        else:\n",
    "            all_splits.append(Document(page_content=current_chunk, metadata=current_metadata))\n",
    "            current_chunk = split + \" \"\n",
    "            current_metadata = {\"source\": doc['source'], \"page\": doc['page']}\n",
    "\n",
    "# Append the last chunk\n",
    "if current_chunk:\n",
    "    all_splits.append(Document(page_content=current_chunk, metadata=current_metadata)) \n",
    "\n",
    "# 3. Vectorstore and Retriever Setup\n",
    "model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=model)\n",
    "llm = ChatOllama(model=\"llama3.1:8b\")  # Or your preferred LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. RAG Function (Incorporating Contextual Compression)\n",
    "def RAG(user_prompt, llm, vectorstore, stream=False, source_summaries=False, retrieval = 'contextual', top_k_hits = 5):\n",
    "    # retrieval methods: contextual, cosine_similarity, both\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(\n",
    "            f\"Source: {doc.metadata['source']} - Page: {doc.metadata.get('page', 'N/A')}\\n\\n{doc.page_content}\" \n",
    "            for doc in docs\n",
    "        )\n",
    "\n",
    "    RAG_TEMPLATE = \"\"\"\n",
    "    You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know.  \n",
    "    \n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \n",
    "    Answer the following question:\n",
    "    \n",
    "    {question}\"\"\"\n",
    "    question = user_prompt\n",
    "    rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    if retrieval == 'contextual' or retrieval == 'both':\n",
    "        compressor = LLMChainExtractor.from_llm(llm) \n",
    "        compression_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=compressor, base_retriever=retriever\n",
    "        )\n",
    "        qa_chain = (\n",
    "            {\"context\": compression_retriever | format_docs, \"question\": RunnablePassthrough()} \n",
    "            | rag_prompt\n",
    "            | llm\n",
    "            | StrOutputParser() \n",
    "        )\n",
    "        \n",
    "        docs = compression_retriever.invoke(question)  # Invoke on the question \n",
    "    if retrieval == 'cosine_similarity' or retrieval == 'both':\n",
    "        if retrieval != 'both':\n",
    "            qa_chain = (\n",
    "                {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "                | rag_prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "        if retrieval == 'both':\n",
    "            docs.extend(vectorstore.similarity_search(question, k = top_k_hits))\n",
    "        else:\n",
    "            docs = vectorstore.similarity_search(question, k = top_k_hits)\n",
    "\n",
    "    if not docs:\n",
    "        return \"No relevant documents found\", pd.DataFrame()\n",
    "\n",
    "    source_data = []\n",
    "    for doc in docs:\n",
    "        source_data.append({\n",
    "            \"source\": doc.metadata['source'], \n",
    "            \"page\": doc.metadata.get('page', 'N/A'),\n",
    "            \"content\": doc.page_content \n",
    "        })\n",
    "\n",
    "    if source_summaries:\n",
    "        summaries = [llm.invoke(f'Summarize this in one or two sentences. Only state main point, nothing else. <{doc.page_content}> ').content for doc in docs]\n",
    "        source_df = pd.DataFrame(source_data)\n",
    "        source_df[\"short summary\"] = summaries\n",
    "    else:\n",
    "        source_df = pd.DataFrame(source_data)\n",
    "\n",
    "    if stream:\n",
    "        for chunk in qa_chain.stream(question):\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "        return '', source_df\n",
    "    else:\n",
    "        result = qa_chain.invoke(question)\n",
    "        return result, source_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Summarize the findings of the human factors study on Helmet Mounted Displays (HMD) for B-1B pilots.\"\n",
    "result, sources_df = RAG(user_prompt, llm, vectorstore, stream=True, source_summaries=True, retrieval = 'both', top_k_hits = 8) \n",
    "print(result)\n",
    "print('\\nSource information:')\n",
    "sources_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
