{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "from unstructured.partition.auto import partition\n",
    "import os\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "# 1. Document Loading and Page Tracking\n",
    "docs = []\n",
    "folder_path = r'C:\\Users\\admin\\Documents\\LLM\\B1-B data'\n",
    "doc_folder = folder_path\n",
    "for filename in os.listdir(doc_folder):\n",
    "    filepath = os.path.join(doc_folder, filename)\n",
    "    if os.path.isfile(filepath):\n",
    "        elements = partition(filename=filepath)\n",
    "        for i, element in enumerate(elements):\n",
    "            text = str(element)\n",
    "            page_number = element.metadata.page_number if element.metadata.page_number else 'N/A'  # Extract page info\n",
    "            docs.append({\"source\": filename, \"content\": text, \"page\": page_number})\n",
    "\n",
    "# 2. Chunking while Preserving Page Information\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4096, chunk_overlap=300)\n",
    "all_splits = []\n",
    "current_chunk = \"\"\n",
    "current_metadata = {}\n",
    "\n",
    "for doc in docs:\n",
    "    splits = text_splitter.split_text(doc['content'])\n",
    "    for split in splits:\n",
    "        if len(current_chunk) + len(split) <= 4096: \n",
    "            current_chunk += split + \" \" \n",
    "            current_metadata = {\"source\": doc['source'], \"page\": doc['page']} \n",
    "        else:\n",
    "            all_splits.append(Document(page_content=current_chunk, metadata=current_metadata))\n",
    "            current_chunk = split + \" \"\n",
    "            current_metadata = {\"source\": doc['source'], \"page\": doc['page']}\n",
    "\n",
    "if current_chunk:\n",
    "    all_splits.append(Document(page_content=current_chunk, metadata=current_metadata)) \n",
    "\n",
    "# 3. Vectorstore and LLM Setup - Load LLM and Vectorstore only once\n",
    "model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=model, collection_name=\"docs_collection\")\n",
    "llm = ChatOllama(model=\"llama3.1:8b\")  # LLM loaded only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ian\\anaconda3\\envs\\RAGv1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28689 documents.\n"
     ]
    }
   ],
   "source": [
    "# if docs.pkl already exists, run this -- run this in demo\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "from unstructured.partition.auto import partition\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import pickle\n",
    "# Load docs from pickle file\n",
    "with open(r'C:\\Users\\Ian\\Documents\\RAG\\B-1B data processed\\docs.pkl', 'rb') as f:\n",
    "    docs = pickle.load(f)\n",
    "\n",
    "# Chunking while Preserving Page Information\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4096, chunk_overlap=300)\n",
    "all_splits = []\n",
    "current_chunk = \"\"\n",
    "current_metadata = {}\n",
    "\n",
    "for doc in docs:\n",
    "    splits = text_splitter.split_text(doc['content'])\n",
    "    for split in splits:\n",
    "        if len(current_chunk) + len(split) <= 4096:\n",
    "            current_chunk += split + \" \"\n",
    "            current_metadata = {\"source\": doc['source'], \"page\": doc['page']}\n",
    "        else:\n",
    "            all_splits.append(Document(page_content=current_chunk, metadata=current_metadata))\n",
    "            current_chunk = split + \" \"\n",
    "            current_metadata = {\"source\": doc['source'], \"page\": doc['page']}\n",
    "\n",
    "# Append the last chunk\n",
    "if current_chunk:\n",
    "    all_splits.append(Document(page_content=current_chunk, metadata=current_metadata))\n",
    "\n",
    "# Vectorstore and Retriever Setup\n",
    "model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "embedding_function = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma.from_documents(all_splits, embedding_function, collection_name=\"B-1B\", persist_directory=r'C:\\Users\\Ian\\Documents\\RAG\\B-1B data processed\\vectorDB_B-1B')\n",
    "llm = ChatOllama(model=\"llama3.1:8b\")  # Or your preferred LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = Document(page_content=\"Humpty Dumpty had a great what?\")\n",
    "query_embedded = Chroma.from_documents(documents=[query], embedding=model, collection_name=\"query_collection\")\n",
    "humpdump = Document(page_content='''                  \n",
    "The story goes: \n",
    "Humpty Dumpty sat on a wall,\n",
    "Humpty Dumpty had a great fall;\n",
    "All the king's horses and all the king's men\n",
    "Couldn't put Humpty together again.                         \n",
    "''')\n",
    "humpdump_embedded = Chroma.from_documents(documents=[humpdump], embedding=model, collection_name=\"humpdumpstory_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: embed query AND documents\n",
    "doc_embeddings = vectorstore._collection.get(include = ['embeddings'])['embeddings']\n",
    "query_embeddings = query_embedded._collection.get(include = ['embeddings'])['embeddings']\n",
    "humpdump_embeddings = humpdump_embedded._collection.get(include = ['embeddings'])['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "# Assuming you have query embedding and document embeddings as numpy arrays\n",
    "# query_embedding: shape (1, embedding_dim)\n",
    "# doc_embeddings: shape (num_docs, embedding_dim)\n",
    "\n",
    "# Combine query and document embeddings\n",
    "\n",
    "all_embeddings = np.vstack([query_embeddings, humpdump_embeddings, doc_embeddings])\n",
    "all_embeddings = normalize(all_embeddings, axis=1)\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# Apply UMAP for dimensionality reduction\n",
    "umap_reducer = umap.UMAP(random_state=42)\n",
    "embeddings_2d = umap_reducer.fit_transform(all_embeddings)\n",
    "\n",
    "# Plot the UMAP result\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot document embeddings (blue)\n",
    "plt.scatter(embeddings_2d[2:, 0], embeddings_2d[2:, 1], c='blue', label='Documents', alpha=0.7)\n",
    "\n",
    "# Plot query embedding (red)\n",
    "plt.scatter(embeddings_2d[0, 0], embeddings_2d[0, 1], c='red', label='Query', s=100)\n",
    "\n",
    "# Plot Humpty Dumpty embedding (green)\n",
    "plt.scatter(embeddings_2d[1, 0], embeddings_2d[1, 1], c='green', label='Humpty Dumpty', marker='o', s=100)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"UMAP Visualization of Query, Humpty Dumpty, and Document Embeddings\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
